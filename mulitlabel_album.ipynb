{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from utils import SRC_DIR\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nn_utils import WeightedBinaryCrossEntropyLoss\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, accuracy_score,\n",
    "    coverage_error, f1_score\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "csv_path = './datasets/cropped_all_one_hot.csv'\n",
    "data = pd.read_csv(csv_path, index_col=0)\n",
    "# convert string labels to list\n",
    "data['labels'] = data['labels'].apply(ast.literal_eval)\n",
    "data = data[data.labels.apply(lambda x: len(x) > 5)]\n",
    "# Extract image paths and labels\n",
    "data['file_names'] = data['file_path'].apply(lambda x: x.split(\"/\")[-1]).tolist()\n",
    "data['galleries'] = data['file_path'].apply(lambda x: x.split(\"/\")[-3]).tolist()\n",
    "data.drop(columns=['file_path', 'labels'], inplace=True)\n",
    "data.shape"
   ],
   "id": "d8f1cc28f8b5741d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = []\n",
    "file_paths = []\n",
    "\n",
    "gallery_names = data['galleries'].unique()\n",
    "\n",
    "grouped_data = data.groupby('galleries')\n",
    "\n",
    "for gallery in tqdm(gallery_names, total=len(gallery_names), desc='Processing galleries'):\n",
    "    temp_df = grouped_data.get_group(gallery)\n",
    "    file_paths.append(temp_df['file_names'].tolist())\n",
    "    temp_labels = temp_df.drop(columns=['file_names', 'galleries']).sum(axis=0).values\n",
    "    labels.append((temp_labels > 0).astype(int))"
   ],
   "id": "d48cb4fd12c167bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = np.array(labels)\n",
    "class_names = data.drop(['file_names', 'galleries'], axis=1).columns.tolist()  # Save the class names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Save class names to a file for future reference\n",
    "with open('class_names.txt', 'w') as f:\n",
    "    for class_name in class_names:\n",
    "        f.write(f\"{class_name}\\n\")\n",
    "\n",
    "print(f\"Class names: {class_names}\")"
   ],
   "id": "b5871cd5e4128317",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the number of occurrences for each class\n",
    "class_counts = np.sum(labels, axis=0)\n",
    "total_samples = labels.shape[0]\n",
    "\n",
    "# Calculate the weights\n",
    "class_weights = total_samples / (num_classes * class_counts)\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "pos_weight = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ],
   "id": "b814b060fb0fd34a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AlbumDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None, max_seq_len=16):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        album_paths = self.file_paths[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        # Apply transformations\n",
    "        processed_album = []\n",
    "        for img_path in album_paths:\n",
    "            img = Image.open(SRC_DIR / img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            processed_album.append(img)\n",
    "\n",
    "        # Padding if necessary\n",
    "        if len(processed_album) < self.max_seq_len:\n",
    "            padding = [torch.zeros_like(processed_album[0])] * (self.max_seq_len - len(processed_album))\n",
    "            processed_album.extend(padding)\n",
    "        else:\n",
    "            processed_album = processed_album[:self.max_seq_len]  # Trim if too long\n",
    "\n",
    "        processed_album = torch.stack(processed_album)\n",
    "        length = min(len(album_paths), self.max_seq_len)\n",
    "\n",
    "        return processed_album, label, length"
   ],
   "id": "ccefbebca32da661",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)\n",
    "\n",
    "    # Pad sequences to the maximum length in this batch\n",
    "    max_len = max(lengths)\n",
    "    padded_images = []\n",
    "    for img_seq in images:\n",
    "        if len(img_seq) < max_len:\n",
    "            padding = [torch.zeros_like(img_seq[0])] * (max_len - len(img_seq))\n",
    "            img_seq = torch.cat([img_seq, torch.stack(padding)])\n",
    "        padded_images.append(img_seq)\n",
    "\n",
    "    padded_images = torch.stack(padded_images)\n",
    "    labels = torch.stack(labels)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return padded_images, labels, lengths\n",
    "\n",
    "\n",
    "# Data augmentation and preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = AlbumDataset(file_paths, labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ],
   "id": "6a3e30888f3c0d8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultilabelAlbumModel(nn.Module):\n",
    "    def __init__(self, num_classes, max_seq_len, lstm_hidden_size=512, lstm_layers=2):\n",
    "        super(MultilabelAlbumModel, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.cnn = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.rnn = nn.LSTM(input_size=2048, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            img = x[:, t, :, :, :]\n",
    "            feature = self.cnn(img)\n",
    "            cnn_features.append(feature)\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(cnn_features, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.rnn(packed_input)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True, total_length=self.max_seq_len)\n",
    "        idx = (lengths - 1).view(-1, 1).expand(len(lengths), rnn_out.size(2)).unsqueeze(1)\n",
    "        last_output = rnn_out.gather(1, idx).squeeze(1)\n",
    "        out = self.fc1(last_output)\n",
    "        out = self.fc2(out)\n",
    "        return torch.sigmoid(out)"
   ],
   "id": "76b1bdfd67283e74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(outputs, targets):\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    outputs = (outputs > 0.5).astype(int)\n",
    "\n",
    "    precision = precision_score(targets, outputs, average='micro')\n",
    "    recall = recall_score(targets, outputs, average='micro')\n",
    "    macro_f1 = f1_score(targets, outputs, average='macro')\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    roc_auc = roc_auc_score(targets, outputs, average='micro')\n",
    "    pr_auc = average_precision_score(targets, outputs, average='micro')\n",
    "    hamming = hamming_loss(targets, outputs)\n",
    "    subset_acc = accuracy_score(targets, outputs)\n",
    "    coverage = coverage_error(targets, outputs)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"hamming_loss\": hamming,\n",
    "        \"subset_accuracy\": subset_acc,\n",
    "        \"coverage_error\": coverage\n",
    "    }"
   ],
   "id": "bc35ea2da01245a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example training loop\n",
    "num_epochs = 10\n",
    "max_seq_len = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MultilabelAlbumModel(num_classes=num_classes, max_seq_len=max_seq_len)\n",
    "model.to(device)\n",
    "\n",
    "# Custom loss function\n",
    "criterion = WeightedBinaryCrossEntropyLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Initialize progress bar for each epoch\n",
    "    with tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for batch in dataloader:\n",
    "            images, labels, lengths = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, lengths)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    metrics = calculate_metrics(all_outputs, all_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Micro F1: {metrics[\"micro_f1\"]:.4f}')\n",
    "    print(f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}, PR AUC: {metrics['pr_auc']:.4f}, Hamming Loss: {metrics['hamming_loss']:.4f}\")\n",
    "    print(f\"Subset Accuracy: {metrics['subset_accuracy']:.4f}, Coverage Error: {metrics['coverage_error']:.4f}\")"
   ],
   "id": "7fbee03c14da5bf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "78fe430e22de489f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
