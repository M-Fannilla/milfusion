{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-16T08:18:13.021421Z",
     "start_time": "2024-06-16T08:18:08.582404Z"
    }
   },
   "source": [
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from utils import SRC_DIR\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nn_utils import WeightedBinaryCrossEntropyLoss\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, accuracy_score,\n",
    "    coverage_error, f1_score\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T08:18:43.121926Z",
     "start_time": "2024-06-16T08:18:36.102086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_path = './datasets/cropped_all_one_hot.csv'\n",
    "data = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "# convert string labels to list\n",
    "data['labels'] = data['labels'].apply(ast.literal_eval)\n",
    "data = data[data.labels.apply(lambda x: len(x) > 5)]\n",
    "\n",
    "# Extract image paths and labels\n",
    "data['file_names'] = data['file_path'].apply(lambda x: x.split(\"/\")[-1]).tolist()\n",
    "data['galleries'] = data['file_path'].apply(lambda x: x.split(\"/\")[-3]).tolist()\n",
    "data.drop(columns=['file_path', 'labels'], inplace=True)\n",
    "data.shape"
   ],
   "id": "d8f1cc28f8b5741d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256465, 91)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T08:18:46.118987Z",
     "start_time": "2024-06-16T08:18:43.123038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = []\n",
    "file_paths = []\n",
    "gallery_names = data['galleries'].unique()\n",
    "grouped_data = data.groupby('galleries')\n",
    "\n",
    "max_seq_len = 0\n",
    "for gallery in tqdm(gallery_names, total=len(gallery_names), desc='Processing galleries'):\n",
    "    temp_df = grouped_data.get_group(gallery)\n",
    "    file_paths.append(temp_df['file_names'].tolist())\n",
    "    max_seq_len = max(max_seq_len, len(temp_df))\n",
    "    temp_labels = temp_df.drop(columns=['file_names', 'galleries']).sum(axis=0).values\n",
    "    labels.append((temp_labels > 0).astype(int))"
   ],
   "id": "d48cb4fd12c167bf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing galleries: 100%|██████████| 16253/16253 [00:02<00:00, 5539.69it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T08:18:46.144236Z",
     "start_time": "2024-06-16T08:18:46.119873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = np.array(labels)\n",
    "class_names = data.drop(['file_names', 'galleries'], axis=1).columns.tolist()  # Save the class names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Save class names to a file for future reference\n",
    "with open('class_names.txt', 'w') as f:\n",
    "    for class_name in class_names:\n",
    "        f.write(f\"{class_name}\\n\")\n",
    "\n",
    "print(f\"Class names: {class_names}\")"
   ],
   "id": "b5871cd5e4128317",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['anal sex', 'anilingus', 'asian', 'ass', 'bath', 'bdsm', 'beach', 'big breasts', 'big woman', 'bikini', 'black penis', 'blonde', 'blowjob', 'bondage', 'boots', 'brunette', 'butt plug', 'chubby', 'clothed', 'cosplay', 'cowgirl (sex position)', 'creampie', 'cum in mouth', 'cumshot', 'cunnilingus', 'curly', 'curvy', 'dildo', 'doggy style (sex position)', 'double penetration', 'dress', 'ebony', 'facial', 'feet', 'fellatio', 'fisting', 'footjob', 'glasses', 'granny', 'group sex', 'gym', 'hairy vulva', 'handjob', 'heels', 'intercourse', 'interracial', 'jeans', 'kissing', 'latina', 'leather', 'lesbian', 'lingerie', 'maid', 'masturbation', 'mature', 'milf', 'missionary (sex position)', 'naked breasts', 'nurse', 'outdoor', 'panties', 'pantyhose', 'petite', 'pissing', 'pool', 'public', 'redhead', 'sandals', 'selfie', 'sex toys', 'shorts', 'shower', 'skinny', 'skirt', 'small breasts', 'smoking', 'socks', 'sports', 'stockings', 'tattoo', 'teacher', 'teen', 'thick', 'thong', 'threesome', 'titjob', 'vagina', 'vulva', 'yoga pants']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T08:18:46.153291Z",
     "start_time": "2024-06-16T08:18:46.145332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the number of occurrences for each class\n",
    "class_counts = np.sum(labels, axis=0)\n",
    "total_samples = labels.shape[0]\n",
    "\n",
    "# Calculate the weights\n",
    "class_weights = total_samples / (num_classes * class_counts)\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "pos_weight = torch.tensor(class_weights, dtype=torch.float32)"
   ],
   "id": "b814b060fb0fd34a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c35dbebd9995b288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_paths_train, file_paths_val, labels_train, labels_val = train_test_split(\n",
    "    file_paths, labels, test_size=0.2, random_state=42\n",
    ")"
   ],
   "id": "3b4a67dd3267ef21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AlbumDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None, max_seq_len=16):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        album_paths = self.file_paths[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        # Apply transformations\n",
    "        processed_album = []\n",
    "        for img_path in album_paths:\n",
    "            img = Image.open(SRC_DIR / img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            processed_album.append(img)\n",
    "\n",
    "        # Padding if necessary\n",
    "        if len(processed_album) < self.max_seq_len:\n",
    "            padding = [torch.zeros_like(processed_album[0])] * (self.max_seq_len - len(processed_album))\n",
    "            processed_album.extend(padding)\n",
    "        else:\n",
    "            processed_album = processed_album[:self.max_seq_len]  # Trim if too long\n",
    "\n",
    "        processed_album = torch.stack(processed_album)\n",
    "        length = min(len(album_paths), self.max_seq_len)\n",
    "\n",
    "        return processed_album, label, length"
   ],
   "id": "ccefbebca32da661",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    images, labels, lengths = zip(*batch)\n",
    "\n",
    "    # Pad sequences to the maximum length in this batch\n",
    "    max_len = max(lengths)\n",
    "    padded_images = []\n",
    "    for img_seq in images:\n",
    "        if len(img_seq) < max_len:\n",
    "            padding = [torch.zeros_like(img_seq[0])] * (max_len - len(img_seq))\n",
    "            img_seq = torch.cat([img_seq, torch.stack(padding)])\n",
    "        padded_images.append(img_seq)\n",
    "\n",
    "    padded_images = torch.stack(padded_images)\n",
    "    labels = torch.stack(labels)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return padded_images, labels, lengths"
   ],
   "id": "6a3e30888f3c0d8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data augmentation and preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AlbumDataset(file_paths_train, labels_train, transform=transform)\n",
    "val_dataset = AlbumDataset(file_paths_val, labels_val, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "8473dcf4dcefa8b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultilabelAlbumModel(nn.Module):\n",
    "    def __init__(self, num_classes, max_seq_len, lstm_hidden_size=512, lstm_layers=2):\n",
    "        super(MultilabelAlbumModel, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.cnn = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.rnn = nn.LSTM(input_size=2048, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            img = x[:, t, :, :, :]\n",
    "            feature = self.cnn(img)\n",
    "            cnn_features.append(feature)\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(cnn_features, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.rnn(packed_input)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True, total_length=self.max_seq_len)\n",
    "        idx = (lengths - 1).view(-1, 1).expand(len(lengths), rnn_out.size(2)).unsqueeze(1)\n",
    "        last_output = rnn_out.gather(1, idx).squeeze(1)\n",
    "        out = self.fc1(last_output)\n",
    "        out = self.fc2(out)\n",
    "        return torch.sigmoid(out)"
   ],
   "id": "76b1bdfd67283e74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(outputs, targets):\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    outputs = (outputs > 0.5).astype(int)\n",
    "\n",
    "    precision = precision_score(targets, outputs, average='micro')\n",
    "    recall = recall_score(targets, outputs, average='micro')\n",
    "    macro_f1 = f1_score(targets, outputs, average='macro')\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    roc_auc = roc_auc_score(targets, outputs, average='micro')\n",
    "    pr_auc = average_precision_score(targets, outputs, average='micro')\n",
    "    hamming = hamming_loss(targets, outputs)\n",
    "    subset_acc = accuracy_score(targets, outputs)\n",
    "    coverage = coverage_error(targets, outputs)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"hamming_loss\": hamming,\n",
    "        \"subset_accuracy\": subset_acc,\n",
    "        \"coverage_error\": coverage\n",
    "    }"
   ],
   "id": "bc35ea2da01245a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example training loop\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MultilabelAlbumModel(num_classes=num_classes, max_seq_len=max_seq_len)\n",
    "model.to(device)\n",
    "\n",
    "# Custom loss function\n",
    "criterion = WeightedBinaryCrossEntropyLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for batch in train_dataloader:\n",
    "            images, labels, lengths = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, lengths)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            all_outputs.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    train_metrics = calculate_metrics(all_outputs, all_labels)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_outputs = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            images, labels, lengths = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            outputs = model(images, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_outputs.append(outputs)\n",
    "            val_labels.append(labels)\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_outputs = torch.cat(val_outputs)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "\n",
    "    val_metrics = calculate_metrics(val_outputs, val_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Train Micro F1: {train_metrics[\"micro_f1\"]:.4f}, Val Micro F1: {val_metrics[\"micro_f1\"]:.4f}')\n",
    "    print(\n",
    "        f\"Train Precision: {train_metrics['precision']:.4f}, Train Recall: {train_metrics['recall']:.4f}, Train Macro F1: {train_metrics['macro_f1']:.4f}\")\n",
    "    print(\n",
    "        f\"Val Precision: {val_metrics['precision']:.4f}, Val Recall: {val_metrics['recall']:.4f}, Val Macro F1: {val_metrics['macro_f1']:.4f}\")\n",
    "    print(\n",
    "        f\"Val ROC AUC: {val_metrics['roc_auc']:.4f}, Val PR AUC: {val_metrics['pr_auc']:.4f}, Val Hamming Loss: {val_metrics['hamming_loss']:.4f}\")\n",
    "    print(\n",
    "        f\"Val Subset Accuracy: {val_metrics['subset_accuracy']:.4f}, Val Coverage Error: {val_metrics['coverage_error']:.4f}\")"
   ],
   "id": "7fbee03c14da5bf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "78fe430e22de489f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
