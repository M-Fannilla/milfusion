{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-15T17:06:51.715288Z",
     "start_time": "2024-06-15T17:06:49.245753Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nn_utils import WeightedBinaryCrossEntropyLoss\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    hamming_loss, accuracy_score,\n",
    "    coverage_error, f1_score\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:06:55.015812Z",
     "start_time": "2024-06-15T17:06:53.426722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the CSV file\n",
    "csv_path = 'datasets/all_one_hot.csv'\n",
    "data = pd.read_csv(csv_path, index_col=0)\n",
    "# Extract image paths and labels\n",
    "file_paths = data['file_path'].tolist()\n",
    "_labels_df = data.drop(columns=['file_path', 'labels'])\n",
    "labels = _labels_df.values\n",
    "class_names = _labels_df.columns.tolist()  # Save the class names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Save class names to a file for future reference\n",
    "with open('class_names.txt', 'w') as f:\n",
    "    for class_name in class_names:\n",
    "        f.write(f\"{class_name}\\n\")\n",
    "\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Calculate the number of occurrences for each class\n",
    "class_counts = np.sum(labels, axis=0)\n",
    "total_samples = labels.shape[0]\n",
    "\n",
    "# Calculate the weights\n",
    "class_weights = total_samples / (num_classes * class_counts)\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "pos_weight = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ],
   "id": "d8f1cc28f8b5741d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['anal sex', 'anilingus', 'asian', 'ass', 'bath', 'bdsm', 'beach', 'big breasts', 'big woman', 'bikini', 'black penis', 'blonde', 'blowjob', 'bondage', 'boots', 'brunette', 'butt plug', 'chubby', 'clothed', 'cosplay', 'cowgirl (sex position)', 'creampie', 'cum in mouth', 'cumshot', 'cunnilingus', 'curly', 'curvy', 'dildo', 'doggy style (sex position)', 'double penetration', 'dress', 'ebony', 'facial', 'feet', 'fellatio', 'fisting', 'footjob', 'glasses', 'granny', 'group sex', 'gym', 'hairy vulva', 'handjob', 'heels', 'intercourse', 'interracial', 'jeans', 'kissing', 'latina', 'leather', 'lesbian', 'lingerie', 'maid', 'masturbation', 'mature', 'milf', 'missionary (sex position)', 'naked breasts', 'nurse', 'outdoor', 'panties', 'pantyhose', 'petite', 'pissing', 'pool', 'public', 'redhead', 'sandals', 'selfie', 'sex toys', 'shorts', 'shower', 'skinny', 'skirt', 'small breasts', 'smoking', 'socks', 'sports', 'stockings', 'tattoo', 'teacher', 'teen', 'thick', 'thong', 'threesome', 'titjob', 'vagina', 'vulva', 'yoga pants']\n",
      "Class counts: [102858   4254  42753 226495  10638  35540   2371 158925  67541  16044\n",
      "    681 137762   6740  33943  14250 203269   1429  67453 123202   6576\n",
      "  80357  12980  15441  86245  17135   6635  67453  47950  80728   1585\n",
      "  13387  18887  27095  52998 106207   2349   2739  22813  13022   5551\n",
      "   2913  70261  26504 108180 109270  12160  12674  31502  43322  12947\n",
      "  47384  77759   9105  66816 103946 189481   3747  87087  11332  29907\n",
      " 132270  12133  59398   2651   9840   4884  43452   1059   3400  48147\n",
      "  24855   3906  71149  79844  86086   6640  13475  12100  55152  42801\n",
      "   3224 124503  67453   4086   5249  13429    666 200259   6434]\n",
      "Class weights: [0.0527897  1.27640872 0.12700495 0.02397334 0.51041951 0.15278117\n",
      " 2.29010658 0.03416607 0.08039328 0.33843447 7.97333729 0.03941466\n",
      " 0.80561464 0.15996944 0.38104159 0.0267126  3.79974996 0.08049816\n",
      " 0.04407268 0.82570601 0.0675715  0.41832378 0.35165097 0.06295835\n",
      " 0.31688606 0.81836363 0.08049816 0.11323968 0.06726096 3.42576826\n",
      " 0.40560564 0.28749101 0.20040017 0.10245373 0.05112509 2.311555\n",
      " 1.98241793 0.23801528 0.41697456 0.97817379 1.86400367 0.07728103\n",
      " 0.2048688  0.05019267 0.04969198 0.44653312 0.42842376 0.17236501\n",
      " 0.12533684 0.41939003 0.11459232 0.06982912 0.59635834 0.08126561\n",
      " 0.05223715 0.0286564  1.44911735 0.06234964 0.47916014 0.18155759\n",
      " 0.0410512  0.4475268  0.09141457 2.04822433 0.55181328 1.1117614\n",
      " 0.12496186 5.12733021 1.59701256 0.11277635 0.21846078 1.3901287\n",
      " 0.0763165  0.06800564 0.06307463 0.81774739 0.40295679 0.44874733\n",
      " 0.09845233 0.12686252 1.68419438 0.04361214 0.08049816 1.32888955\n",
      " 1.03445279 0.40433708 8.15291696 0.0271141  0.84392955]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:06:59.117069Z",
     "start_time": "2024-06-15T17:06:59.106246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AlbumDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None, max_seq_len=16):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        album_paths = self.file_paths[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        # Apply transformations\n",
    "        processed_album = []\n",
    "        for img_path in album_paths:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            processed_album.append(img)\n",
    "\n",
    "        # Padding if necessary\n",
    "        if len(processed_album) < self.max_seq_len:\n",
    "            padding = [torch.zeros_like(processed_album[0])] * (self.max_seq_len - len(processed_album))\n",
    "            processed_album.extend(padding)\n",
    "\n",
    "        processed_album = torch.stack(processed_album)\n",
    "        length = min(len(album_paths), self.max_seq_len)\n",
    "\n",
    "        return processed_album, label, length"
   ],
   "id": "ccefbebca32da661",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:00.579440Z",
     "start_time": "2024-06-15T17:07:00.576292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data augmentation and preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = AlbumDataset(file_paths, labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: (\n",
    "    torch.stack([item[0] for item in x]),\n",
    "    torch.stack([item[1] for item in x]),\n",
    "    torch.tensor([item[2] for item in x])\n",
    "))"
   ],
   "id": "6a3e30888f3c0d8c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:01.073464Z",
     "start_time": "2024-06-15T17:07:01.061298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultilabelAlbumModel(nn.Module):\n",
    "    def __init__(self, num_classes, max_seq_len):\n",
    "        super(MultilabelAlbumModel, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.rnn = nn.LSTM(input_size=2048, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(512 * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            img = x[:, t, :, :, :]\n",
    "            feature = self.cnn(img)\n",
    "            cnn_features.append(feature)\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(cnn_features, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.rnn(packed_input)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True, total_length=self.max_seq_len)\n",
    "        idx = (lengths - 1).view(-1, 1).expand(len(lengths), rnn_out.size(2)).unsqueeze(1)\n",
    "        last_output = rnn_out.gather(1, idx).squeeze(1)\n",
    "        out = self.fc1(last_output)\n",
    "        out = self.fc2(out)\n",
    "        return torch.sigmoid(out)"
   ],
   "id": "76b1bdfd67283e74",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(outputs, targets):\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    outputs = (outputs > 0.5).astype(int)\n",
    "\n",
    "    precision = precision_score(targets, outputs, average='micro')\n",
    "    recall = recall_score(targets, outputs, average='micro')\n",
    "    macro_f1 = f1_score(targets, outputs, average='macro')\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    roc_auc = roc_auc_score(targets, outputs, average='micro')\n",
    "    pr_auc = average_precision_score(targets, outputs, average='micro')\n",
    "    hamming = hamming_loss(targets, outputs)\n",
    "    subset_acc = accuracy_score(targets, outputs)\n",
    "    coverage = coverage_error(targets, outputs)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"hamming_loss\": hamming,\n",
    "        \"subset_accuracy\": subset_acc,\n",
    "        \"coverage_error\": coverage\n",
    "    }\n",
    "\n",
    "\n",
    "# Example training loop with additional metrics\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 10\n",
    "max_seq_len = 24\n",
    "model = MultilabelAlbumModel(num_classes=num_classes, max_seq_len=max_seq_len)\n",
    "criterion = WeightedBinaryCrossEntropyLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, labels, lengths = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, lengths)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        all_outputs.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    metrics = calculate_metrics(all_outputs, all_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Micro F1: {metrics[\"micro_f1\"]:.4f}')\n",
    "    print(\n",
    "        f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "    print(\n",
    "        f\"ROC AUC: {metrics['roc_auc']:.4f}, PR AUC: {metrics['pr_auc']:.4f}, Hamming Loss: {metrics['hamming_loss']:.4f}\")\n",
    "    print(f\"Subset Accuracy: {metrics['subset_accuracy']:.4f}, Coverage Error: {metrics['coverage_error']:.4f}\")"
   ],
   "id": "7fbee03c14da5bf6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
