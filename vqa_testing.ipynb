{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# git clone https://github.com/M-Fannilla/milfusion.git && cd milfusion && pip install -r requirements.txt",
   "id": "49a3f355c95bb0ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# cd /workspace\n",
    "# curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-480.0.0-linux-x86_64.tar.gz\n",
    "# tar -xf google-cloud-cli-480.0.0-linux-x86_64.tar.gz\n",
    "# ./google-cloud-sdk/install.sh\n",
    "# ./google-cloud-sdk/bin/gcloud init"
   ],
   "id": "885a542e542b1861",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# cd /workspace && mkdir images\n",
    "# gsutil -m cp \"gs://chum_bucket_stuff/images.zip.partaa\" \"gs://chum_bucket_stuff/images.zip.partab\" \"gs://chum_bucket_stuff/images.zip.partac\" \"gs://chum_bucket_stuff/images.zip.partad\" \"gs://chum_bucket_stuff/images.zip.partae\" \"gs://chum_bucket_stuff/images.zip.partaf\" /workspace\n",
    "# apt update && apt install unzip\n",
    "# cat images.zip.part* > images.zip\n",
    "# cd images && unzip -q images.zip"
   ],
   "id": "c833ea831070d41c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# huggingface-cli download llava-hf/llava-v1.6-vicuna-7b-hf\n",
    "# huggingface-cli download llava-hf/llava-v1.6-mistral-7b-hf\n",
    "# huggingface-cli download llava-hf/llava-v1.6-vicuna-13b-hf\n",
    "# huggingface-cli download OpenGVLab/Mini-InternVL-Chat-4B-V1-5"
   ],
   "id": "899d994b9637f36",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a42f47b5ceed9a61",
   "metadata": {},
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from utils import SRC_DIR\n",
    "from typing import Callable\n",
    "from prompts.prompts import *\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "labels_df = pd.read_csv(\"datasets/cropped_all_one_hot.csv\", index_col = 0)\n",
    "labels_df.drop(['good_image'], axis = 1, inplace = True)\n",
    "labels_df['file_path'] = labels_df['file_path'].apply(lambda x: x.replace(\"images\", \"/workspace/images\"))\n",
    "\n",
    "# DROP SEX LABELS\n",
    "labels_df.drop(['cowgirl (sex position)', 'doggy style (sex position)', 'missionary (sex position)'], axis = 1, inplace = True)\n",
    "all_cols = labels_df.columns.to_list()\n",
    "all_cols.remove('file_path')\n",
    "all_cols.remove('labels')\n",
    "cols_map = np.array(labels_df.columns[2:])\n",
    "\n",
    "def regen_labels(row):\n",
    "    return cols_map[row.values.astype(bool)]\n",
    "    \n",
    "labels_df['labels'] = labels_df.drop(['file_path', 'labels'], axis = 1).apply(regen_labels, axis = 1)\n",
    "#####\n",
    "\n",
    "labels_df['labels_len'] = labels_df['labels'].apply(lambda x: len(x))\n",
    "labels_df = labels_df[labels_df['labels_len'] > 5]\n",
    "labels_df.drop(['labels_len'], axis =1, inplace = True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "labels_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9384db22eca929b4",
   "metadata": {},
   "source": [
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "# model_name = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "# model_name = \"llava-hf/llava-v1.6-vicuna-13b-hf\"\n",
    "\n",
    "if \"vicuna\" in model_name:\n",
    "    processor = LlavaNextProcessor.from_pretrained(model_name,  padding_side='left')\n",
    "else:\n",
    "    processor = LlavaNextProcessor.from_pretrained(model_name)\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ").to(\"cuda\").eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9db7ae9e916a5a4f",
   "metadata": {},
   "source": [
    "def batch_results(answers):\n",
    "    def extract_list(input_string: str):\n",
    "        input_string = input_string.replace(\"\\n\", \"\")\n",
    "        pattern = r'\\[(.*?)\\]'\n",
    "        matches = re.findall(pattern, input_string)\n",
    "        if len(matches) > 0:\n",
    "            matches_list = matches[0].strip().replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
    "            return [m.strip() for m in matches_list]  # proper list return\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    dirty = [extract_list(ans.split(\"[/INST]\" if \"mistral\" in model_name else \"ASSISTANT\")[-1]) for ans in answers]\n",
    "    out = []\n",
    "    for D in dirty:\n",
    "        out.append([x.strip() for x in D])\n",
    "    return out\n",
    "\n",
    "def convert_to_model_query(prompt: str):\n",
    "    if \"mistral\" in model_name:\n",
    "        return f\"[INST] <image>\\n{prompt} [/INST]\"\n",
    "    elif \"vicuna\" in model_name:\n",
    "        return f\"USER: <image>\\n{prompt} ASSISTANT:\"\n",
    "    else:\n",
    "        return f\"<|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\n{prompt}<|im_end|><|im_start|>assistant\\n\"\n",
    "\n",
    "\n",
    "def batch_loader(dataframe: pd.DataFrame, batch_size: int, dynamic_prompt: Callable):\n",
    "    for i in range(0, len(dataframe), batch_size):\n",
    "        temp_df = dataframe.iloc[i:i + batch_size]\n",
    "        file_paths = temp_df.file_path.tolist()\n",
    "        labels = temp_df.labels.tolist()\n",
    "\n",
    "        images = [Image.open(SRC_DIR / file_path) for file_path in file_paths]\n",
    "        prompts = [convert_to_model_query(dynamic_prompt(label)) for label in labels]\n",
    "        yield file_paths, images, prompts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f149efd1007a90a",
   "metadata": {},
   "source": [
    "result_folder = Path(\"./llava_results\")\n",
    "result_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "def get_json_path(batch_number: int, prompt: Callable):\n",
    "    _p = result_folder / prompt.__name__ / f\"{batch_number}.json\"\n",
    "    _p.parent.mkdir(exist_ok=True, parents=True)\n",
    "    return _p\n",
    "\n",
    "\n",
    "def get_last_batch():\n",
    "    return max(\n",
    "        [\n",
    "            int(x.stem)\n",
    "            for x in result_folder.iterdir()\n",
    "            if x.is_file() and not x.as_posix().endswith(\".json\")\n",
    "        ] + [0]\n",
    "    )\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "prompts = [\n",
    "    simple_original, simple_0, simple_1, simple_2, simple_3, simple_4,\n",
    "    medium_original, medium_0, medium_1, medium_2, medium_3, medium_4,\n",
    "]\n",
    "\n",
    "for P in tqdm(prompts, total=len(prompts), desc=\"Batching prompts...\"):\n",
    "    for i, (file_paths, images, prompts) in enumerate(batch_loader(test_df, batch_size, P)):\n",
    "        inputs = processor(prompts, images=images, return_tensors=\"pt\", padding=True).to(\"cuda:0\")\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "        result = processor.batch_decode(output, skip_special_tokens=True)\n",
    "        res = batch_results(result)\n",
    "    \n",
    "        result_dict = {\n",
    "            file_path: res[i]\n",
    "            for i, file_path in enumerate(file_paths)\n",
    "        }\n",
    "    \n",
    "        json_path = get_json_path(i, simple_original)\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(result_dict, f)\n",
    "\n",
    "print(f\"{model_name} took {(time.time() - start) / 60}mins to complete\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "result_folder = Path(\"/workspace/milfusion/llava_results\")\n",
    "\n",
    "\n",
    "def get_json_path(batch_number: int, prompt: Callable):\n",
    "    _p = result_folder / prompt.__name__ / f\"{batch_number}.json\"\n",
    "    _p.parent.mkdir(exist_ok=True, parents=True)\n",
    "    return _p\n",
    "\n",
    "\n",
    "def plot_images_with_labels(version: int, prompt_list: List[Callable] = prompts):\n",
    "    image_paths = None\n",
    "    all_labels = {}\n",
    "\n",
    "    for P in prompt_list:\n",
    "        with open(get_json_path(version, P), 'r') as f:\n",
    "            data_simple = json.load(f)\n",
    "        all_labels[P.__name__] = list(data_simple.values())  # Use the name of the function as the key\n",
    "        if image_paths is None:\n",
    "            image_paths = list(data_simple.keys())\n",
    "\n",
    "    num_images = len(image_paths)\n",
    "    num_columns = 1\n",
    "    num_rows = num_images\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(10, num_rows * 5))\n",
    "\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        img = plt.imread(img_path)\n",
    "        axes[i].imshow(img)\n",
    "\n",
    "        _title = \"\"\n",
    "        for fn_name, labels in all_labels.items():\n",
    "            _T = f\"{fn_name}: {labels[i]}\\n\"\n",
    "            _title += _T\n",
    "\n",
    "        axes[i].set_title(_title, fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = result_folder / f\"prompt_results_{version}.jpg\"\n",
    "    out_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    plt.savefig(out_path)"
   ],
   "id": "2be006dc5699cb5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for x in range(13):\n",
    "    plot_images_with_labels(version=x)"
   ],
   "id": "d0068c367855f2d0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
